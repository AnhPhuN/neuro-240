[100,20,240,40] "Arc File"
[260,20,430,50] "Edit View"
[450,20,550,50] "Spaces"
[580,20,660,50] "Tabs"
[680,20,960,50] "Archive Extensions"
[320,130,440,180] "MDPI"
[320,280,610,310] "Search for Articles:"
[740,280,900,310] "Title / Keyword"
[990,20,1090,40] "Window"
[1120,20,1190,50] "Help"
[2300,20,2340,40] "US"
[2540,20,2590,50] "Q"
[2730,20,2990,40] "Fri Apr 14 10:37 AM"
[910,140,1020,170] "Journals"
[1100,140,1180,170] "Topics"
[1260,150,1400,170] "Information"
[1480,150,1680,170] "Author Services"
[1750,150,2030,170] "Initiatives About"
[2290,140,2480,180] "Sign In / Sign Up"
[2560,150,2640,170] "Submit"
[1130,280,1400,310] "Author / Affiliation / Email"
[1530,290,1720,320] "Applied Sciences"
[1930,290,2100,310] "All Article Types"
[2340,280,2430,310] "Search"
[2500,290,2620,310] "Advanced"
[320,400,1200,430] "Journals / Applied Sciences / Volume 10 / Issue 1 / 10.3390/app10010391"
[950,540,980,570] "K"
[2130,540,2380,570] "Order Article Reprints"
[440,560,820,620] "applied sciences"
[450,690,690,720] "Submit to this Journal"
[450,770,690,800] "Review for this Journal"
[460,850,680,880] "Edit a Special Issue"
[320,940,560,980] "Article Menu"
[320,1040,560,1060] "Subscribe SciFeed"
[320,1130,610,1150] "Recommended Articles"
[320,1210,530,1240] "Related Info Link"
[780,1220,790,1240] "V"
[320,1300,610,1330] "More by Authors Links"
[780,1310,800,1320] "V"
[320,1500,470,1530] "Article Views"
[770,1500,820,1530] "3527"
[320,1590,430,1610] "Citations"
[810,1590,820,1610] "2"
[320,1760,560,1790] "Table of Contents"
[780,1770,800,1780] "1"
[350,1830,440,1860] "Abstract"
[50,1880,340,1900] "Loading web-font TeX/Main/Bold"
[350,1870,480,1900] "Introduction"
[350,1920,500,1940] "Related Work"
[2600,600,2650,620] "Share"
[1030,640,1260,670] "Open Access Article"
[2620,660,2630,700] "-."
[1020,710,2190,750] "Panoptic Segmentation-Based Attention for Image Captioning"
[2610,720,2650,740] "Help"
[1020,780,2190,830] "by & Wenjie Cai 1 (D Zheng Xiong 1 , 8 Xianfang Sun 2 (D, @ Paul L. Rosin 2 & (D Longcun Jin 1,* Ä°D and"
[1380,810,1600,850] "8 Xinyi Peng 1"
[2610,840,2650,860] "Cite"
[1040,890,2220,920] "1 School of Software Engineering, South China University of Technology, Guangzhou 510006, China"
[1030,930,2070,970] "2 School of Computer Science and Informatics, Cardiff University, Cardiff CF10 3AT, UK"
[2580,960,2680,1010] "Discuss in SciProfiles"
[1030,980,1710,1010] "* Author to whom correspondence should be addressed."
[1020,1070,1800,1100] "Appl. Sci. 2020, 10(1), 391; https://doi.org/10.3390/app10010391"
[2590,1110,2670,1130] "Endorse"
[1020,1130,2120,1200] "Received: 12 December 2019 / Revised: 30 December 2019 / Accepted: 1 January 2020 / Published: 4 January 2020"
[2610,1180,2650,1200] "..."
[1030,1230,1900,1260] "(This article belongs to the Section Computing and Artificial Intelligence)"
[2580,1230,2670,1250] "Comment"
[1080,1300,1210,1330] "Download"
[1320,1300,1510,1330] "Browse Figures"
[1590,1300,1780,1330] "Review Reports"
[1870,1300,2050,1330] "Versions Notes"
[1020,1460,1130,1490] "Abstract"
[1020,1530,2400,1920] "Image captioning is the task of generating textual descriptions of images. In order to obtain a better image representation, attention mechanisms have been widely adopted in image captioning. However, in existing models with detection-based attention, the rectangular attention regions are not fine-grained, as they contain irrelevant regions (e.g., background or overlapped regions) around the object, making the model generate inaccurate captions. To address this issue, we propose panoptic segmentation-based attention that performs attention at a mask-level (i.e., the shape of the main part of an instance). Our approach extracts feature vectors from the corresponding segmentation regions, which is more fine-grained than current attention mechanisms. Moreover, in order to process features of different classes independently, we propose a dual-attention module which is generic and can be applied to other frameworks. Experimental results showed that our model could recognize the overlapped objects and understand the scene better. Our approach achieved competitive performance against state-of-the-art methods. We"
